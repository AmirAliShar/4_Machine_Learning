{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dependancies\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross-validation Accuracy: 0.7991529597163399\n",
      "Test Accuracy / Predication: 0.8379888268156425\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.8061952132374668\n",
      "Test Accuracy / Predication: 0.7988826815642458\n",
      "\n",
      "Model: XGBoost\n",
      "Cross-validation Accuracy: 0.8076233625529401\n",
      "Test Accuracy / Predication: 0.7932960893854749\n",
      "\n",
      "Model: LogisticRegression\n",
      "Cross-validation Accuracy: 0.7977839062346105\n",
      "Test Accuracy / Predication: 0.8100558659217877\n",
      "\n",
      "Model: Support vector machine \n",
      "Cross-validation Accuracy: 0.8160248202501723\n",
      "Test Accuracy / Predication: 0.8044692737430168\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', RandomForestClassifier(random_state=42))]) with accuracy 0.8379888268156425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variable\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42)),\n",
    "    (\"LogisticRegression\",LogisticRegression(random_state=42)),\n",
    "    (\"Support vector machine \",SVC(random_state=42)),\n",
    "    \n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    \n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the performance metrics\n",
    "    print(\"Model:\", name)\n",
    "    print(\"Cross-validation Accuracy:\", mean_accuracy)\n",
    "    print(\"Test Accuracy / Predication:\", accuracy)\n",
    "    print()\n",
    "    \n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "# Retrieve the best model\n",
    "print(\"Best Model:\", best_model,\"with accuracy\",best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other  method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  LinearRegression\n",
      "MSE = 0.035\n",
      "R2 score = 0.500 \n",
      "\n",
      "Model =  SVR\n",
      "MSE = 0.038\n",
      "R2 score = 0.465 \n",
      "\n",
      "Model =  KNN\n",
      "MSE = 0.056\n",
      "R2 score = 0.209 \n",
      "\n",
      "Model =  Decision Tree\n",
      "MSE = 0.063\n",
      "R2 score = 0.109 \n",
      "\n",
      "Model =  random forest\n",
      "MSE = 0.044\n",
      "R2 score = 0.375 \n",
      "\n",
      "Model =  GradientBoostingRegressor\n",
      "MSE = 0.044\n",
      "R2 score = 0.383 \n",
      "\n",
      "Model =  XGBRegressor\n",
      "MSE = 0.045\n",
      "R2 score = 0.364 \n",
      "\n",
      "Best Model:  LinearRegression() with accuracy = 0.5002 \n"
     ]
    }
   ],
   "source": [
    "#For regression\n",
    "#Import the library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For models \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Load the data set \n",
    "df=sns.load_dataset(\"tips\")\n",
    "\n",
    "#Step = 2\n",
    "#Preprocessing the data\n",
    "\n",
    "#Other method for label encoder\n",
    "for i in df.columns:\n",
    "    df[i]=LabelEncoder().fit_transform(df[i])\n",
    "    #Apply the standardscaler on  total bill and tips \n",
    "    #Scaler decrease the MSE \n",
    "    df[[\"total_bill\",\"tip\"]]=MinMaxScaler().fit_transform(df[[\"total_bill\",\"tip\"]])#This is best second Max absolute\n",
    "    \n",
    "    #step =3\n",
    "    #separate the feature and target value\n",
    "    X=df.drop('tip',axis=1)\n",
    "    Y=df['tip']\n",
    "    \n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    #Step =4 \n",
    "    #Create a dictionaries of list of models to evaluate performance\n",
    "\n",
    "models={\n",
    "    \n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"SVR\":SVR(),\n",
    "    \"KNN\":KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Decision Tree\":DecisionTreeRegressor(random_state=42),\n",
    "    \"random forest\":RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoostingRegressor\":GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBRegressor\":XGBRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "#Steps=5\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "model_best=None\n",
    "best_accuracy = 0\n",
    "for name,model in models.items():\n",
    "    #Train the model\n",
    "    model.fit(X_train,Y_train)\n",
    "    \n",
    "    #Predict the model\n",
    "    Y_pred=model.predict(X_test)\n",
    "    mse=mean_squared_error(Y_test,Y_pred)\n",
    "    r2=r2_score(Y_test,Y_pred)\n",
    "    #Evaluate the model\n",
    "    print(\"Model = \",name)\n",
    "    print(f\"MSE = {mse:.3f}\")\n",
    "    print(f\"R2 score = {r2:.3f} \")\n",
    "    print()\n",
    "    \n",
    "    # Check if the current model has the best accuracy\n",
    "    if r2 > best_accuracy:\n",
    "        best_accuracy = r2\n",
    "        model_best = model\n",
    "\n",
    "# Retrieve the best model\n",
    "print(\"Best Model: \", model_best,f\"with accuracy = {best_accuracy:.04f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "model  LogisticRegression \n",
      "test accuracy =  1.0\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  0  74]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  SVC\n",
      "test accuracy =  0.994413407821229\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       105\n",
      "           1       1.00      0.99      0.99        74\n",
      "\n",
      "    accuracy                           0.99       179\n",
      "   macro avg       1.00      0.99      0.99       179\n",
      "weighted avg       0.99      0.99      0.99       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  1  73]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  KNeighborsClassifier\n",
      "test accuracy =  0.9553072625698324\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       105\n",
      "           1       0.92      0.97      0.95        74\n",
      "\n",
      "    accuracy                           0.96       179\n",
      "   macro avg       0.95      0.96      0.95       179\n",
      "weighted avg       0.96      0.96      0.96       179\n",
      "\n",
      "confusion matrix \n",
      " [[99  6]\n",
      " [ 2 72]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  Decision Tree classifier\n",
      "test accuracy =  1.0\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  0  74]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  RandomForestClassifier\n",
      "test accuracy =  1.0\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  0  74]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  XGBClassifier\n",
      "test accuracy =  1.0\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  0  74]]\n",
      "*************************************************\n",
      "*******************************************\n",
      "model  Gradient boost classifier\n",
      "test accuracy =  1.0\n",
      "classfication report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       105\n",
      "           1       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           1.00       179\n",
      "   macro avg       1.00      1.00      1.00       179\n",
      "weighted avg       1.00      1.00      1.00       179\n",
      "\n",
      "confusion matrix \n",
      " [[105   0]\n",
      " [  0  74]]\n",
      "*************************************************\n",
      "Best model  GradientBoostingClassifier() acuuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "#For regression\n",
    "#Import the library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#import the data set\n",
    "data=sns.load_dataset(\"titanic\")\n",
    "\n",
    "#Step =2\n",
    "#Preprocessing the data\n",
    "\n",
    "#Remove the deck data\n",
    "data.drop(\"deck\",axis=1,inplace=True)\n",
    "\n",
    "#Fill the missing value\n",
    "data[\"age\"]=data[\"age\"].fillna(data[\"age\"].mean())\n",
    "for i in data.columns:\n",
    "    if data[i].dtype==\"category\" or data[i].dtype=='object':\n",
    "        data[i]=data[i].fillna(data[i].mode()[0])\n",
    "        data[i]=LabelEncoder().fit_transform(data[i])\n",
    "        data[['age','fare']]=MinMaxScaler().fit_transform(data[['age','fare']])\n",
    "#step 3\n",
    "\n",
    "#Train test and split the data\n",
    "X=data.drop(\"survived\",axis=1)\n",
    "Y=data['survived']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "# step=4\n",
    "\n",
    "#Create  dictonary of the model\n",
    "\n",
    "model={\n",
    "    \"LogisticRegression \":LogisticRegression(random_state=42),\n",
    "    \"SVC\":SVC(),\n",
    "    \"KNeighborsClassifier\":KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree classifier\":DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\":RandomForestClassifier(),\n",
    "    \"XGBClassifier\":XGBClassifier(),\n",
    "    \"Gradient boost classifier\":GradientBoostingClassifier()\n",
    "    \n",
    "}\n",
    "#Step=6\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "best_accuracy=0.0\n",
    "best_model=None\n",
    "for name,models in model.items():\n",
    "    \n",
    "    #Train the model\n",
    "    models.fit(X_train,Y_train)\n",
    "    \n",
    "    #Predict the model\n",
    "    Y_pred=models.predict(X_test)\n",
    "    \n",
    "    #evaluate \n",
    "    test_accuracy=accuracy_score(Y_test,Y_pred)\n",
    "    \n",
    "    class_report=classification_report(Y_test,Y_pred)\n",
    "    \n",
    "    conf_matrix=confusion_matrix(Y_test,Y_pred)\n",
    "    \n",
    "    print(\"*******************************************\")\n",
    "    print(\"model \",name)\n",
    "    print(\"test accuracy = \",test_accuracy)\n",
    "    print(\"classfication report \\n\",class_report)\n",
    "    print(\"confusion matrix \\n\",conf_matrix)\n",
    "    print(\"*************************************************\")\n",
    "        \n",
    "if test_accuracy>best_accuracy:\n",
    "    best_accuracy=test_accuracy\n",
    "    best_model=models\n",
    "print(\"Best model \",best_model, \"acuuracy = \",test_accuracy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
